{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/programminghistorian/ph-submissions/blob/gh-pages/assets/non-english-and-multilingual-text-analysis/non-english-and-multilingual-text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load our text file so we can use it with our analysis packages."
      ],
      "metadata": {
        "id": "DNgjNEO7Rv7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open the file and assign it to the variable named \"war_and_peace\" so we can reference it later on\n",
        "# then, print the contents of the file to make sure it was read correctly\n",
        "\n",
        "# we are using a minimally pre-processed excerpt of the novel for the purposes of this tutorial\n",
        "with open(\"sample_data/war_and_peace_excerpt.txt\") as file:\n",
        "    war_and_peace = file.read()\n",
        "    print(war_and_peace)"
      ],
      "metadata": {
        "id": "_LOAFsQSR4UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's clean out the newline characters to make the text easier to work with computationally. We won't worry too much about thoroughly cleaning the text for the purposes of this tutorial, since we will primarily focus on our analysis methods rather than pre-processing. For a good introduction to preparing your text for multilingual analysis, please consult [this article.](https://modernlanguagesopen.org/articles/10.3828/mlo.v0i0.294)"
      ],
      "metadata": {
        "id": "fwqd0C63XDky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing newlines (\"\\n\") with a space, assigning the cleaned text to a new variable, then printing it\n",
        "cleaned_war_and_peace = war_and_peace.replace(\"\\n\", \" \")\n",
        "print(cleaned_war_and_peace)"
      ],
      "metadata": {
        "id": "ll121R6sXHGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've read the file, let's begin to process it. First, we install our packacges-NLTK, spaCy, and Stanza--using pip and import them."
      ],
      "metadata": {
        "id": "MQ_Es02vTM6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the packages, if you haven't already\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install stanza"
      ],
      "metadata": {
        "id": "fyFHaWrWRhdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the code is imported, let's split the text into sentences and detect each sentence's language. We'll begin by tokenizing using NLTK.\n",
        "\n",
        "\n",
        "There are different sentence tokenizers included in the NLTK package. NLTK recommends using the PunktSentenceTokenizer for a language specified by the user (reference: https://www.nltk.org/api/nltk.tokenize.html), but if working with multilingual text this may not be the best approach, as you will be applying a tokenization model targeted at one language to all of the languages in your text. For the purposes of this tutorial, we will use the sent_tokenize method built into NLTK without specifying a language."
      ],
      "metadata": {
        "id": "mJzvqe3zTsFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, let's install the 'punkt' resources required to use the tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# then we import the sent_tokenize method and apply it to our war_and_peace variable\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk_sent_tokenized = sent_tokenize(cleaned_war_and_peace)\n",
        "# if you were going to specify a language, the following syntax would be used: nltk_sent_tokenized = sent_tokenize(war_and_peace, language=\"russian\")"
      ],
      "metadata": {
        "id": "70qPzG-RUoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entire text is now accessible as a list of sentences within the variable nltk_sent_tokenized. Let's print three sentences we'll be working with: one entirely in Russian, one entirely in French, and one that is in both languages:"
      ],
      "metadata": {
        "id": "NNAd125wdFVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Russian: Так говорила в июле 1805 года известная Анна Павловна Шерер, фрейлина и приближенная императрицы Марии Феодоровны, встречая важного и чиновного князя Василия, первого приехавшего на ее вечер.\n",
        "# French: Non, je vous préviens, que si vous ne me dites pas, que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j’y crois) — je ne vous connais plus, vous n’êtes plus mon ami, vous n’êtes plus мой верный раб, comme vous dites.\n",
        "# Multilang: Je vois que je vous fais peur, садитесь и рассказывайте.\n",
        "\n",
        "# Russian only\n",
        "rus_sent = nltk_sent_tokenized[5]\n",
        "print('Russian: ' + rus_sent)\n",
        "\n",
        "# French only\n",
        "fre_sent = nltk_sent_tokenized[2]\n",
        "print('French: ' + fre_sent)\n",
        "\n",
        "# both French and Russian\n",
        "multi_sent = nltk_sent_tokenized[4]\n",
        "print('Multilang: ' + multi_sent)"
      ],
      "metadata": {
        "id": "rzzklcUcdGrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's perform the same sentence tokenization using spaCy, grabbing the same sample of three sentences."
      ],
      "metadata": {
        "id": "rIrHLbccgMi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# downloading our multilingual sentence tokenizer\n",
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "# loading the multilingual sentence tokenizer we just downloaded\n",
        "nlp = spacy.load(\"xx_sent_ud_sm\")\n",
        "# applying the spaCy model to our text variable\n",
        "doc = nlp(cleaned_war_and_peace)\n",
        "\n",
        "# assigning the tokenized sentences to a list so it's easier for us to manipulate them later\n",
        "spacy_sentences = list(doc.sents)\n",
        "\n",
        "print(spacy_sentences)"
      ],
      "metadata": {
        "id": "L9W2XNo4gMKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assign our sentences to variables, like we did with NLTK. spaCy returns its sentences not as strings, but as spaCy tokens. In order to print them as we did with the NLTK sentences above, we'll need to convert them to strings in order to concatenate them with the prefix identifying their language:"
      ],
      "metadata": {
        "id": "UJqLiclednJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Russian only\n",
        "spacy_rus_sent = str(spacy_sentences[5])\n",
        "print('Russian: ' + spacy_rus_sent)\n",
        "\n",
        "# French only\n",
        "spacy_fre_sent = str(spacy_sentences[2])\n",
        "print('French: ' + spacy_fre_sent)\n",
        "\n",
        "# both French and Russian\n",
        "spacy_multi_sent = str(spacy_sentences[4])\n",
        "print('Multilang: ' + spacy_multi_sent)"
      ],
      "metadata": {
        "id": "WDTWOk2fDEaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see above that both models tokenized the sentences in the same way, as the NLTK and spaCy indices match to the same sentences from the text. Now, let's perform the same operation with Stanza, using its built-in multilingual pipeline."
      ],
      "metadata": {
        "id": "tdyH1waVyZpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "from stanza.pipeline.multilingual import MultilingualPipeline\n",
        "\n",
        "nlp = MultilingualPipeline(processors='tokenize')\n",
        "doc = nlp(cleaned_war_and_peace)\n",
        "# printing all sentences\n",
        "#print([sentence.text for sentence in doc.sentences])"
      ],
      "metadata": {
        "id": "wc50W8CgyhBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's find the same sentences we did with NLTK and spaCy above.\n",
        "First, let's add the sentence tokens to a list, converting them to strings.\n",
        "This makes it easier for us to look at specific sentences by their indices."
      ],
      "metadata": {
        "id": "X3PH-RWOhGM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza_sentences = []\n",
        "for sentence in doc.sentences:\n",
        "  stanza_sentences.append(sentence.text)\n",
        "\n",
        "# Russian only\n",
        "stanza_rus_sent = str(stanza_sentences[5])\n",
        "print('Russian: ' + stanza_rus_sent)\n",
        "\n",
        "# French only\n",
        "stanza_fre_sent = str(stanza_sentences[2])\n",
        "print('French: ' + stanza_fre_sent)\n",
        "\n",
        "# both French and Russian\n",
        "stanza_multi_sent = str(stanza_sentences[4])\n",
        "print('Multilang: ' + stanza_multi_sent)"
      ],
      "metadata": {
        "id": "l4evPJT5hJ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have three sentences to use as examples, we can begin to perform our analysis of each one. First, let's detect the language of each sentence computationally, starting with the monolingual examples.\n",
        "\n",
        "NLTK has the TextCat module for language identification using the TextCat algorithm; let's try that on our sentences below."
      ],
      "metadata": {
        "id": "oUogacnvfNh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading an NLTK corpus reader required by the TextCat module\n",
        "nltk.download('crubadan')\n",
        "\n",
        "# loading the TextCat package and applying it to each of our sentences\n",
        "tcat = nltk.classify.textcat.TextCat()\n",
        "rus_estimate = tcat.guess_language(rus_sent)\n",
        "fre_estimate = tcat.guess_language(fre_sent)\n",
        "multi_estimate = tcat.guess_language(multi_sent)\n",
        "\n",
        "# printing the results\n",
        "print(rus_estimate)\n",
        "print(fre_estimate)\n",
        "print(multi_estimate)"
      ],
      "metadata": {
        "id": "2opuDPxfflhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, TextCat correctly identified the Russian and French sentences. Since it can't output more than one language per sentence, it guessed Russian for our multilingual sentence. We'll examine other ways to handle language detection for multilingual sentences after we perform our sentence classification using spaCy and Stanza."
      ],
      "metadata": {
        "id": "Pw4uLUBmT5xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's look at language classification for sentences using spaCy\n",
        "# first, we install the spacy_langdetect package from the Python Package Index, then we import it\n",
        "# and use it to detect our languages\n",
        "\n",
        "!pip install spacy_langdetect\n",
        "\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "\n",
        "# setting up our language detector to work with spaCy\n",
        "def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "\n",
        "Language.factory(\"language_detector\", func=get_lang_detector)\n",
        "nlp.add_pipe('language_detector', last=True)\n",
        "\n",
        "# running the language detection on each sentence and printing the results\n",
        "rus_doc = nlp(spacy_rus_sent)\n",
        "print(rus_doc._.language)\n",
        "\n",
        "fre_doc = nlp(spacy_fre_sent)\n",
        "print(fre_doc._.language)\n",
        "\n",
        "multi_doc = nlp(spacy_multi_sent)\n",
        "print(multi_doc._.language)"
      ],
      "metadata": {
        "id": "t0nGXfS9Rjh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got similar, expected results with spaCy; note the confidence printed after the language guess is far lower for the multilingual sentence given that it contains more than one language. Now let's try Stanza, which has a built-in language identifier."
      ],
      "metadata": {
        "id": "NiEkzC4ta533"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpJLye2VMPol"
      },
      "outputs": [],
      "source": [
        "# importing our models required for language detection\n",
        "from stanza.models.common.doc import Document\n",
        "from stanza.pipeline.core import Pipeline\n",
        "\n",
        "# setting up our pipeline\n",
        "nlp = Pipeline(lang=\"multilingual\", processors=\"langid\")\n",
        "\n",
        "# specifying which sentences to run the detection on, then running the detection code\n",
        "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
        "docs = [Document([], text=text) for text in docs]\n",
        "nlp(docs)\n",
        "\n",
        "# printing the text of each sentence alongside the language estimates\n",
        "print(\"\\n\".join(f\"{doc.text}\\t{doc.lang}\" for doc in docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Stanza classified the final sentence as French, deviating from the other models.\n",
        "\n",
        "For multilingual sentences, classifying both languages within the same sentence is not a simple problem to solve, and requires more granular analysis than a sentence-by-sentence approach. One method to detect all of the languages contained in a single sentence, for example, would be to break the sentence into its component words and then try to detect the language of each word--which will have questionable accuracy, given that we are only looking at one word at a time--and then group consecutive words of the same language within the string into new strings consisting only of one language. For this example, we can also detect non-Roman script and split the string into its component languages that way. Here is a simple implementation:"
      ],
      "metadata": {
        "id": "-1dG1HGfcNm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, we split the sentence into its component words\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "tokenized_sent = wordpunct_tokenize(multi_sent)\n",
        "\n",
        "# then, we check if each word contains Cyrillic characters, and split the string into two strings of Cyrillic and non-Cyrillic script\n",
        "# for simplicity's sake, we'll omit any punctuation in this example\n",
        "\n",
        "# importing the regex package so we can use a regular expression\n",
        "import regex\n",
        "# importing the string package to detect punctuation\n",
        "from string import punctuation\n",
        "\n",
        "# setting empty lists we will later populate with our words\n",
        "cyrillic_words = []\n",
        "latin_words = []\n",
        "\n",
        "# iterating through our sentence and using regex to detect Cyrillic characters\n",
        "# if Cyrillic is found, we append the word to our cyrillic_words list; otherwise, we append the word to the Latin list\n",
        "# if a tokenized word is only punctuation, we continue without appending\n",
        "for word in tokenized_sent:\n",
        "  if regex.search(r'\\p{IsCyrillic}', word):\n",
        "    cyrillic_words.append(word)\n",
        "  else:\n",
        "    if word in punctuation:\n",
        "       continue\n",
        "    else:\n",
        "        latin_words.append(word)\n",
        "\n",
        "\n",
        "print(cyrillic_words)\n",
        "print(latin_words)\n",
        "\n",
        "# we can then join these lists into strings to run our language detection code\n",
        "cyrillic_only_list = ' '.join(cyrillic_words)\n",
        "latin_only_list = ' '.join(latin_words)\n",
        "\n",
        "print(cyrillic_only_list)\n",
        "print(latin_only_list)\n",
        "\n",
        "# now we use TextCat again to detect their languages\n",
        "tcat = nltk.classify.textcat.TextCat()\n",
        "multi_estimate_1 = tcat.guess_language(cyrillic_only_list)\n",
        "multi_estimate_2 = tcat.guess_language(latin_only_list)\n",
        "\n",
        "# printing our estimates\n",
        "print(multi_estimate_1)\n",
        "print(multi_estimate_2)\n"
      ],
      "metadata": {
        "id": "-9OnbJPJdyKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's perform part-of-speech tagging for our sentences using spaCy and Stanza.\n",
        "\n",
        "NLTK does not support POS tagging on languages other than English out-of-the-box, but you can train your own model using a corpus to tag languages other than English. Documentation on the tagger, and how to train your own, can be found here: https://www.nltk.org/book/ch05.html#sec-n-gram-tagging"
      ],
      "metadata": {
        "id": "4pWCuQwrcbA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tagging our sentences with spaCy is very straightforward. Since we know we're working with Russian and French, we can download the appropriate spaCy corpora and use them to get the proper POS tags for the words in our sentences. The syntax remains the same regardless of which language model we use."
      ],
      "metadata": {
        "id": "3dAKLFOocqpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# French first\n",
        "\n",
        "# downloading our French and Russian corpora from spaCy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "\n",
        "# loading the corpus\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "doc = nlp(spacy_fre_sent)\n",
        "# printing the text of each word and its POS tag\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "# And doing the same with our Russian sentence\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(spacy_rus_sent)\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "id": "dAOA3znqazDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For multilingual text, we can use the words we generated earlier to tag each language separately, then join the words back into a complete sentence again."
      ],
      "metadata": {
        "id": "8pBNNI4Ud45s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We split our sentence into Russian and French words as before, only this time we preserve the punctuation\n",
        "# by appending any punctuation we encounter to the last list we appended to, preserving the proper placement\n",
        "# of our punctuation (e.g., it will append a period to the word it should follow).\n",
        "\n",
        "cyrillic_words = []\n",
        "latin_words = []\n",
        "# initializing a blank string to keep track of the last list we appended to\n",
        "last_appended_list = ''\n",
        "\n",
        "for word in tokenized_sent:\n",
        "  if regex.search(r'\\p{IsCyrillic}', word):\n",
        "    cyrillic_words.append(word)\n",
        "    # updating our string to track the list we appended a word to\n",
        "    last_appended_list = 'cyr'\n",
        "  else:\n",
        "    # handling punctuation\n",
        "    if word in punctuation:\n",
        "        if last_appended_list == 'cyr':\n",
        "            cyrillic_words.append(word)\n",
        "        elif last_appended_list == 'lat':\n",
        "            latin_words.append(word)\n",
        "    else:\n",
        "        latin_words.append(word)\n",
        "        last_appended_list = 'lat'\n",
        "\n",
        "print(cyrillic_words)\n",
        "print(latin_words)\n",
        "\n",
        "# We can then join these lists into strings to run our language detection on them.\n",
        "# We'll use a regular expression to remove the extra whitespace before each punctuation mark\n",
        "# that was created when we tokenized the sentence into words. This will preserve the punctuation\n",
        "# as it was present in the original sentence.\n",
        "\n",
        "cyrillic_only_list = ' '.join(cyrillic_words)\n",
        "latin_only_list = ' '.join(latin_words)\n",
        "\n",
        "cyr_no_extra_space = regex.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', cyrillic_only_list)\n",
        "lat_no_extra_space = regex.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', latin_only_list)\n",
        "\n",
        "print(cyr_no_extra_space)\n",
        "print(lat_no_extra_space)\n",
        "\n",
        "# Finally, we can tag each list of words using the appropriate language model.\n",
        "# loading the corpus\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "doc = nlp(lat_no_extra_space)\n",
        "# printing the text of each word and its POS tag\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "# And doing the same with our Russian sentence\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(cyr_no_extra_space)\n",
        "print(doc.text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n"
      ],
      "metadata": {
        "id": "XWdswImYfbJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's perform POS tagging using Stanza. The syntax is quite straightforward:"
      ],
      "metadata": {
        "id": "Ys4UiqTQUFVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# French\n",
        "# loading our pipeline and applying it to our sentence\n",
        "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos')\n",
        "doc = nlp(stanza_fre_sent)\n",
        "\n",
        "# printing our words and POS tags\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
        "\n",
        "# Russian\n",
        "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos')\n",
        "doc = nlp(stanza_rus_sent)\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "K7nKTIZrUn50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now take a simpler approach for the multilingual analysis than we did with spaCy, as Stanza's multilingual pipeline allows us to return POS tags with similar syntax to the examples above."
      ],
      "metadata": {
        "id": "sUYdRqkoVL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports so we can use Stanza's MultilingualPipeline\n",
        "from stanza.models.common.doc import Document\n",
        "from stanza.pipeline.core import Pipeline\n",
        "from stanza.pipeline.multilingual import MultilingualPipeline\n",
        "\n",
        "# running the multilingual pipeline on our French, Russian, and multilingual sentences simultaneously\n",
        "nlp = MultilingualPipeline(processors='tokenize,pos')\n",
        "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
        "nlp(docs)\n",
        "print(*[f'word: {word.text}\\tupos: {word.upos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "metadata": {
        "id": "wWwyxGrLiNt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's perform lemmatization on our sentences using spaCy and Stanza (NLTK does not provide out-of-the-box lemmatization for non-English languages). Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
        "\n",
        "spaCy does not have a single multingual lemmatization corpus, so we'll have to run separate models on our Russian and French text and split our multilingual sentence into its component parts again."
      ],
      "metadata": {
        "id": "uk8w3iEtltG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy\n",
        "# More info, including supported languages, can be found here: https://spacy.io/api/lemmatizer\n",
        "\n",
        "\n",
        "# French\n",
        "fre_nlp = spacy.load(\"fr_core_news_sm\")\n",
        "doc = nlp(spacy_fre_sent)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.lemma_)\n",
        "\n",
        "# Russian\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(spacy_rus_sent)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.lemma_)\n",
        "\n",
        "# Multilingual\n",
        "# We'll run each model on the chunks of text we split apart earlier.\n",
        "\n",
        "# Russian\n",
        "doc = nlp(cyr_no_extra_space)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.lemma_)\n",
        "\n",
        "# French\n",
        "doc = nlp(lat_no_extra_space)\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.lemma_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "adcSunW5mUwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now in Stanza."
      ],
      "metadata": {
        "id": "YX_iCIP1mWIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this syntax is very similar to the POS tagging with the multilingual pipeline we used earlier\n",
        "from stanza.models.common.doc import Document\n",
        "from stanza.pipeline.core import Pipeline\n",
        "from stanza.pipeline.multilingual import MultilingualPipeline\n",
        "\n",
        "# adding the 'lemma' processor to the pipeline and running it on our sentences\n",
        "nlp = MultilingualPipeline(processors='tokenize,lemma')\n",
        "docs = [stanza_rus_sent, stanza_fre_sent, stanza_multi_sent]\n",
        "nlped_docs = nlp(docs)\n",
        "# iterating through each sentence and printing the lemmas\n",
        "for doc in nlped_docs:\n",
        "  lemmas = [word.lemma for t in doc.iter_tokens() for word in t.words]\n",
        "  print(lemmas)"
      ],
      "metadata": {
        "id": "aWeIvXFCmXLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}