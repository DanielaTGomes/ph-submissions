{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e713dd2-d261-43d6-a074-e014f164b894",
   "metadata": {},
   "source": [
    "Notebook to accompany the Programming Historian lesson:\n",
    "    \n",
    "    \"Web scraping of news articles from the UK Web Archive using Boilerpipe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a7c81-aaed-4d24-a63a-7b49ab9e2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cd8b8-06a2-46ec-af4b-9f84cc82446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata = pd.read_csv('./data/export.csv', skiprows=2)\n",
    "\n",
    "# In jupyter notebooks, putting the name of a variable on the last line\n",
    "# of the cell will show that variable below the cell. This helps to inspect\n",
    "# the values and structures of the variables your algorithms are working on.\n",
    "shinedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ad3cf-c9f4-40c2-85c8-78787bc64ef2",
   "metadata": {},
   "source": [
    "### Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c688e2c-ca3f-429f-9285-d2cacf4e724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the columns\n",
    "shinedata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218670fa-fa7c-4756-b35a-dd9a3fe394b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = shinedata.columns.to_list()\n",
    "cols = [c.strip().replace(' ', '_') for c in cols]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aed06-bbaa-4d92-9d8a-63ebda922ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols[7] = 'Archive_URL'  # change item 8 in the list\n",
    "cols[-1] = 'Original_URL'  # change the last item in the list\n",
    "shinedata.columns = cols  # Assign the list of column names to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943a802-3c79-47d9-be77-372f197ca11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6f3e0-c2d8-40fd-8053-bfe525c05fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata = shinedata[['Crawl_Date', 'Archive_URL', 'Original_URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253f24f-88b8-4f57-ba12-1aea64af646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a443745-6fbe-43c2-a6e6-9d19ea235d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata = shinedata.drop_duplicates(subset='Original_URL', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37bb4c-da1d-468a-bc9f-9bb8c23421ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shinedata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feca68b-a43f-4acb-827b-31ae89f3a224",
   "metadata": {},
   "source": [
    "### Deeper data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54215e3-4394-4f4c-9ef5-eb9d1e4f3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = shinedata.Archive_URL\n",
    "urls = [url for url in urls if url.endswith('rss') == False]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc291a-2ed2-4ec3-a51e-3ab33fd9b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write list to file - this is useful if you want to use wget, trafilatura or another command line scraping tool.\n",
    "with open('./data/unique-urls.txt', 'w') as f:\n",
    "    f.write('\\n'.join(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b3777-f536-4895-a5e4-59fe2baab348",
   "metadata": {},
   "source": [
    "## Web Scraping using Boilerpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b505f-f0ff-4c1e-b4b8-8b38db43275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boilerpy3\n",
    "from boilerpy3 import extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fd3fa-a343-4c1e-8e63-191b207e033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = extractors.ArticleExtractor()\n",
    "content = extractor.get_content_from_url(urls[55])\n",
    "\n",
    "content  # View what Boilerpipe returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f36d29-5685-43a8-a4cc-4461ebeff341",
   "metadata": {},
   "source": [
    "### Choosing filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdeeda-0315-4dba-ab0a-e1212ce66b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace characters in a URL with filesystem safe characters. Returns a string.\n",
    "\n",
    "def filenameFromUrl(url):\n",
    "    return url.replace(\"http://\", \"\").replace(\"https://\", \"\").replace(\".\", \"_\").replace(\"/\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172aef4-05f3-4f64-805d-71f10c8de536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it out on the first 5 URL's in our list.\n",
    "for url in urls[:5]:\n",
    "    print(filenameFromUrl(url[27:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f438e7-9022-4d32-8f75-9fe2d26b5a95",
   "metadata": {},
   "source": [
    "### Where to save our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728e604-86cd-4a1a-89b6-9a7dc3689441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpusdir = './data/corpus/'\n",
    "\n",
    "if not os.path.exists(corpusdir):\n",
    "    os.mkdir(corpusdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3ffd0-72c7-4139-98a7-cb8a076a0e53",
   "metadata": {},
   "source": [
    "**How long is scraping 361 URL's going to take?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf4480-dd1c-472d-b3dc-5e2ebfc4572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "extractor = extractors.ArticleExtractor()\n",
    "content = extractor.get_content_from_url(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad225071-3a6e-4881-a1cf-0495c8267df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls) * 2.8 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a39ee6-3d74-4861-baba-53839130e931",
   "metadata": {},
   "source": [
    "### Scraping all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca47f57-c040-432b-bdce-ae78ba1aa677",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    filename = os.path.join(corpusdir, filenameFromUrl(url)) + '.txt'\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w', encoding='utf8') as f:\n",
    "            print('Scraping... {}'.format(url))\n",
    "            extractor = extractors.ArticleExtractor()\n",
    "            content = extractor.get_content_from_url(url)\n",
    "            f.write(content)\n",
    "    else:\n",
    "        print('Already scraped... {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cda0f9-991c-433a-a148-83a6051aa0c8",
   "metadata": {},
   "source": [
    "Below is the same scraping algorithm with a small bit of error handling added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8e339-059b-47ee-98af-90bf73557a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "errorlog = 'error.log'\n",
    "\n",
    "for url in urls:\n",
    "    filename = os.path.join(corpusdir, filenameFromUrl(url)) + '.txt'\n",
    "    if not os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf8') as f:\n",
    "                print('Scraping... {}'.format(url))\n",
    "                extractor = extractors.ArticleExtractor()\n",
    "                content = extractor.get_content_from_url(url)\n",
    "                f.write(content)\n",
    "\n",
    "        except Exception as ex:\n",
    "            errormsg = 'Exception of type {} on... {}\\n'.format(type(ex).__name__, url)\n",
    "            print(errormsg)\n",
    "            with open(errorlog, 'a', encoding='utf8') as errlog:\n",
    "                # note we are opening this file with the 'a' status, \n",
    "                # which means append to existing if we opened it with 'w'\n",
    "                # which is much more common, that would overwrite the file.\n",
    "                errlog.write(errormsg)\n",
    "\n",
    "    else:\n",
    "        print('Already scraped... {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c364a25-4d99-4c68-a735-64c736b7ed42",
   "metadata": {},
   "source": [
    "## Are the articles still relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89fc7b-2853-4cb4-b25c-d7f7b1b40335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Let's first create a new directory called 'filtered_corpus' where the\n",
    "# files originated from the 'corpus' directory will be stored after filtered. \n",
    "corpusdir = './data/corpus'\n",
    "filteredcorpusdir = './data/filtered_corpus'\n",
    "\n",
    "if not os.path.exists(filteredcorpusdir):\n",
    "    os.mkdir(filteredcorpusdir)\n",
    "\n",
    "\n",
    "items = ['legacy', 'Legacy']\n",
    "\n",
    "\n",
    "# For every text file in the 'corpus', if file contain the words \n",
    "# in the list of items, copy and paste to the new directory 'filtered_corpus'.\n",
    "# Also, print whether the words were found or not in each text file and \n",
    "# how many times they are cited. \n",
    "for filename in os.listdir(corpusdir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(corpusdir, filename), 'r', encoding='utf-8') as myfile:\n",
    "            content = myfile.read()\n",
    "\n",
    "            for i in items:\n",
    "                lis = re.findall(i, content)\n",
    "                if len(lis)==0:\n",
    "                    print(filename,'Not found')\n",
    "                    \n",
    "                elif len(lis)==1:\n",
    "                    print(filename,'Found once')\n",
    "                    with open(os.path.join(filteredcorpusdir, filename + '.txt'), 'w', encoding='utf-8') as file1:\n",
    "                        file1.write(content)\n",
    "                        \n",
    "                elif len(lis)==2:\n",
    "                    print(filename,'Found twice')\n",
    "                    with open(os.path.join(filteredcorpusdir, filename + '.txt'), 'w', encoding='utf-8') as file1:\n",
    "                        file1.write(content)\n",
    "                        \n",
    "                else:\n",
    "                    print(filename,'Found', len(lis), 'times')\n",
    "                    with open(os.path.join(filteredcorpusdir, filename + '.txt'), 'w', encoding='utf-8') as file1:\n",
    "                        file1.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e2363-7d3b-4120-ade7-f357a63bb485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
